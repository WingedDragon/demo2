1 工具类库

开发环境：Anaconda (Anaconda自带以下库Numpy、Scipy、Matplotlib、Pandas、和Scikit-Learn)

科学计算库：
Numpy：提供数组支持，以及相应的高效处理函数；
Scipy：提供矩阵支持，以及矩阵相关的数值计算模块；
Matplotlib：数据可视化工具，作图库；
Pandas：强大、灵活的数据分析和探索工具；
Scikit - Learn：支持回归、分类、聚类等强大的机器学习库

分布式：
spark-ml

2 挖掘算法

  （2-1）根据学习方式的分类

    监督学习：
        输入的数据为训练数据，并且每一个数据都会带有标签，比如“广告/非广告”，或者当时的股票的价格。
        通过训练过程建模，模型需要作出预测，如果预测出错会被修正。
        直到模型输出准确的训练结果，训练过程会一直持续。常用于解决问题有分类和回归。
        常用的算法包括 逻辑回归 和 BP神经网络

    无监督学习：
        输入的标签没有数据，输出没有标准答案，就是一系列的样本。
        无监督学习通过推断输入数据中的结构建模。这可能是提取一般规律，
        可以是通过数学处理系统系统的减少冗杂，或者根据相似性组织数据。
        常用于解决的问题有聚类，降维和关联规则的学习。
        常用的算法包括了 Apriori算法 和 K均值算法

    半监督学习：
        半监督学习的输入数据包含标签和不带标签的样本。
        半监督学习的情况是有一个预期中的预测，但是模型必须通过学习结构整理数据从而做出预测。
        常用于解决的问题是分类和回归。
        常用的算法是对所有的无标签的数据建模进行的预测算法（可以看做无监督学习的延伸）


    (2-2) 算法


    1：回归算法：
        回归分析是研究自变量和因变量之间关系的一种预测模型技术。
        这些技术应用于预测时间序列模型和找到变量之间的关系。
        回归分析也是一种常用的统计方法，经由统计机器学习融入机器学习领域。

        常用的回归算法包括：
            普通最小二乘回归（OLSR），
                单自变量的线性回归
            线性回归，
                线性回归是利用连续性变量来估计实际数值（例如房价，呼叫次数和总销售额等）
            逻辑回归，
                逻辑回归其实是一个分类算法而不是回归算法。通常是利用已知的自变量来预测一个离散型因变量的值（像二进制值0/1，是/否，真/假）。简单来说，它就是通过拟合一个逻辑函数（logit fuction）来预测一个事件发生的概率。所以它预测的是一个概率值，自然，它的输出值应该在0到1之间

        例子：
            * 线性回归：Y= ax+b (房价)
            * 逻辑回归： 判断是否问题
            房型  面积  楼层  位置  装修  价格
            3    150   8    3环   是    1000

    2：分类算法：
        单一的分类方法主要包括：决策树、贝叶斯、人工神经网络、K-近邻、支持向量机和基于关联规则的分类等；另外还有用于组合单一分类方法的集成学习算法，如Bagging和Boosting等。

    （1）k-近邻
            k-近邻(kNN，k-Nearest Neighbors)算法是一种基于实例的分类方法，KNN是通过测量不同特征值之间的距离进行分类
            该方法就是找出与未知样本x距离最近的k个训练样本，看这k个样本中多数属于哪一类，就把x归为那一类。
            k-近邻方法是一种懒惰学习方法，它存放样本，直到需要分类时才进行分类，如果样本集比较复杂，
            可能会导致很大的计算开销，因此无法应用到实时性很强的场合。

            k为近邻的数量(k=3 为空间距离最近的3个样本的分类，然后确定比例较大的分类为该样本分类)
            例子：
                电影名称    谈话时间    打斗时间    电影分类
                a          50         10        文艺
                b          20         45        动作
                x          30         35         ?
     （2）决策树
             * 决策树学习是以实例为基础的归纳学习算法，它着眼于从一组无次序、无规则的实例中推理出以决策树表示的分类规则
             * 决策树是一种用于对实例进行分类的树形结构。决策树由节点（node）和有向边（directed edge）组成。
             * 节点的类型有两种：内部节点和叶子节点。
                 其中，内部节点表示一个特征或属性的测试条件（用于分开具有不同特性的记录），叶子节点表示一个分类。

             主要的决策树算法（CART树是二叉树，而ID3和C4.5可以是多叉树）
                 ID3、C4.5（C5.0）、
                 CART、
                 PUBLIC、
                 SLIQ
                 SPRINT
             示例：
                 医生看病（未明确疼痛位置，判断体温 -> 血液 -> 血压）
                 约会对象（性格 -> 颜值 -> 职业 -> ...）

    （3）随机森林(random forest) > svm > 决策树

        随机森林顾名思义，是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。
        在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，
        看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类

        随机森林有许多优点：

            具有极高的准确率
            随机性的引入，使得随机森林不容易过拟合
            随机性的引入，使得随机森林有很好的抗噪声能力
            能处理很高维度的数据，并且不用做特征选择
            既能处理离散型数据，也能处理连续型数据，数据集无需规范化
            训练速度快，可以得到变量重要性排序
            容易实现并行化

        随机森林的缺点：

            当随机森林中的决策树个数很多时，训练时需要的空间和时间会较大
            随机森林模型还有许多不好解释的地方，有点算个黑盒模型

    （2）贝叶斯
        贝叶斯（Bayes）分类算法是一类利用概率统计知识进行分类的算法，
        如朴素贝叶斯（Naive Bayes）算法。

        这些算法主要利用Bayes定理来预测一个未知类别的样本属于各个类别的可能性，
        选择其中可能性最大的一个类别作为该样本的最终类别。
        由于贝叶斯定理的成立本身需要一个很强的条件独立性假设前提，
        而此假设在实际情况中经常是不成立的，因而其分类准确性就会下降。
        为此就出现了许多降低独立性假设的贝叶斯分类算法，
        如TAN（Tree Augmented Na?ve Bayes)算法，
        它是在贝叶斯网络结构的基础上增加属性对之间的关联来实现的。

        贝叶斯定理离不开条件概率
        条件概率定义：
        事件A在另外一个事件B已经发生条件下的发生概率。条件概率表示为P（A|B），读作“在B条件下A的概率”

        示例：
            生活中，我们经常知道这种情况 ，P(A|B),但是不知道P(B|A).比如：
            A:   表示用户收入高
            B：表示订购2G流量套餐

            P(A|B) 表示订购2G流量套餐的用户收入高的概率，这个可以通过统计的样本算出得到。
            但是现在有一个用户收入高（A），他购买2G流量套餐(B)的概率是多少，即P(B|A)

    （3）人工神经网络
        人工神经网络（Artificial Neural Networks，ANN）是一种应用类似于大脑神经突触联接的结构进行信息处理的数学模型。
        在这种模型中，大量的节点（或称”神经元”，或”单元”）之间相互联接构成网络，即”神经网络”，以达到处理信息的目的。
        神经网络通常需要进行训练，训练的过程就是网络进行学习的过程。
        训练改变了网络节点的连接权的值使其具有分类的功能，经过训练的网络就可用于对象的识别。
        神经网络已有上百种不同的模型，
        常见的有
            BP网络、
            径向基RBF网络、
            Hopfield网络、
            随机神经网络（Boltzmann机）、
            竞争神经网络（Hamming网络，自组织映射网络）等。
        但是当前的神经网络仍普遍存在收敛速度慢、计算量大、训练时间长和不可解释等缺点。


    （5）支持向量机
        支持向量机（SVM，Support Vector Machine）是Vapnik根据统计学习理论提出的一种新的学习方法 ，
        它的最大特点是根据结构风险最小化准则，以最大化分类间隔构造最优分类超平面来提高学习机的泛化能力，
        较好地解决了非线性、高维数、局部极小点等问题。
        对于分类问题，支持向量机算法根据区域中的样本计算该区域的决策曲面，由此确定该区域中未知样本的类别。

    （6）基于关联规则的分类
        关联规则学习在数据不同变量之间观察到一些关联，算法要做的就是找到这些关联中最能描述的规则，
        也就是获取一个时间和其他事件之间的依赖和关联的知识。

        关联分类方法一般由两步组成：
            第一步用关联规则挖掘算法从训练数据集中挖掘出所有满足指定支持度和置信度的类关联规则；
            第二步使用启发式方法从挖掘出的类关联规则中挑选出一组高质量的规则用于分类。

        决策树算法：
            决策树算法的目标是根据数据属性的实际值，创建一个预测样本目标值的模型。
            训练时，树状的结构会不断地分叉，知道做出最终的决策。决策树算法常用于分类和回归。

        常用的关联规则算法有：
            Apriori算法，
            Eclat算法



    3：聚类算法：
        聚类和回归一样，既可以用来形容一类问题，也可以用来代指一组方法。
        聚类的方法通常涉及质心（centroid-based）或者层次（hierarchal）等建模方式，
        所有的方法都和固有的数据结构相关，目标是将数据按照他们之间共有的最大组织方式聚集在一起。
        换句话说，算法将输入的样本聚集成围绕一些中心的数据团，通过这样的方式发现的数据分布结构中的规律。

        常用的聚类算法包括：
            K-均值，
            K-中位数，
            EM算法，
            分层聚类算法

        k-means
                算法描述
                    输入：簇的数目k；包含n个对象的数据集D。
                    输出：k个簇的集合。

            方法：
                从D中任意选择k个对象作为初始簇中心；
                repeat;
                根据簇中对象的均值，将每个对象指派到最相似的簇；
                更新簇均值，即计算每个簇中对象的均值；
                计算准则函数；
                until准则函数不再发生变化。

            算法的性能分析
               1）优点
                    （1）k-平均算法是解决聚类问题的一种经典算法，算法简单、快速。
                    （2）对处理大数据集，该算法是相对可伸缩的和高效率的，因为它的复杂度大约是O（nkt），其中n是所有对象的数目，k是簇的数目,t是迭代的次数。通常k<<n。这个算法经常以局部最优结束。
                    （3）算法尝试找出使平方误差函数值最小的k个划分。当簇是密集的、球状或团状的，而簇与簇之间区别明显时，它的聚类效果很好。
               2）缺点
                    （1）k-平均方法只有在簇的平均值被定义的情况下才能使用，不适用于某些应用，如涉及有分类属性的数据不适用。
                    （2）要求用户必须事先给出要生成的簇的数目k。
                    （3）对初值敏感，对于不同的初始值，可能会导致不同的聚类结果。
                    （4）不适合于发现非凸面形状的簇，或者大小差别很大的簇。
                    （5）对于"噪声"和孤立点数据敏感，少量的该类数据能够对平均值产生极大影响。

            算法的改进
                一是数据预处理(样本数据进行正规化处理防止某些大值属性的数据左右样本间的距离)
                二是初始聚类中心选择
                    (在选择初始聚类中心时，先将孤立点纳入统计范围，在样本中计算对象两两之间的距离，
                    选出距离最大的两个点作为两个不同类的聚类中心，
                    接着从其余的样本对象中找出已经选出来的所有聚类中心的距离和最大的点为另一个聚类中心，
                    直到选出k个聚类中心)
                三是迭代过程中聚类种子的选择。
                1、首先对样本数据进行正规化处理，这样就能防止某些大值属性的数据左右样本间的距离。
                   给定一组含有n个数据的数据集，每个数据含有m个属性，分别计算每一个属性的均值、标准差对每条数据进行标准化。
                2、其次，初始聚类中心的选择对最后的聚类效果有很大的影响，原K-means算法是随机选取k个数据作为聚类中心，
                   而聚类的结果要是同类间尽可能相似，不同类间尽可能相异

            聚类中心选好以后，就要进行不断的迭代计算，在K-means算法中，
            是将聚类均值点(类中所有数据的几何中心点)作为新的聚类种子进行新一轮的聚类计算，
            在这种情况下，新的聚类种子可能偏离真正的数据密集区，从而导致偏差，
            特别是在有孤立点存在的情况下，有很大的局限性。在选择初始中心点时，由于将孤立点计算在内，
            所以在迭代过程中要避免孤立点的影响。


            * 对孤立点的改进—基于距离法
            基于距离法移除孤立点, 具体过程如下:
            首先扫描一次数据集, 计算每一个数据对象与其临近对象的距离, 累加求其距离和, 并计算出距离和均值。
            如果某个数据对象的距离和大于距离和均值, 则视该点为孤立点。
            把这个对象从数据集中移除到孤立点集合中, 重复直到所有孤立点都找到。最后得到新的数据集就是聚类的初始集合。

            * 对随机选取初始聚类中心的改进
              二分Kmeans是一个贪心算法，假设要将整个数据聚为n簇，那么我们可以将整个数据当做一簇，使用普通Kmeans来聚为两簇；
              然后从这两簇中挑选一簇（可以像决策树那样，使用一定的评价手段如SSE等来进行筛选，选择SSE最大的）
              出来再次使用普通Kmeans聚为两簇；然后整个数据被聚为了三簇，继续从这三簇中挑选一簇（可以像决策树那样，使用一定的评价手段如SSE等来进行筛选）
              进行普通Kmeans聚类......直到整个数据被聚为n簇
              【度量聚类效果的指标是SSE(Sum of Squared Error，误差平方和）】



参考：
    1 从决策树到随机森林：
        https://baijiahao.baidu.com/s?id=1574420750567758&wfr=spider&for=pc
    2 朴素bayes
        https://blog.csdn.net/amds123/article/details/70173402